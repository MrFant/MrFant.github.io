<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 算法 | YII Fant's Blog]]></title>
  <link href="https://mrfant.github.io/blog/categories/suan-fa/atom.xml" rel="self"/>
  <link href="https://mrfant.github.io/"/>
  <updated>2018-09-03T08:37:50+08:00</updated>
  <id>https://mrfant.github.io/</id>
  <author>
    <name><![CDATA[Yii]]></name>
    <email><![CDATA[yii.fant@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[大数阶乘问题及内存分配的堆和栈区别]]></title>
    <link href="https://mrfant.github.io/blog/2017/11/24/the-big-num-mul/"/>
    <updated>2017-11-24T17:19:52+08:00</updated>
    <id>https://mrfant.github.io/blog/2017/11/24/the-big-num-mul</id>
    <content type="html"><![CDATA[<h2>问题描述：</h2>

<blockquote><p>问题描述</p>

<p>　　输入一个正整数<em>n</em>，输出<em>n</em>!的值。
　　其中<em>n</em>!=1<em>2</em>3<em>…**n</em>。</p>

<p>算法描述</p>

<p>　　<em>n</em>!可能很大，而计算机能表示的整数范围有限，需要使用高精度计算的方法。使用一个数组<em>A</em>来表示一个大整数<em>a</em>，<em>A</em>[0]表示<em>a</em>的个位，<em>A</em>[1]表示<em>a</em>的十位，依次类推。
　　将<em>a</em>乘以一个整数<em>k</em>变为将数组<em>A</em>的每一个元素都乘以<em>k</em>，请注意处理相应的进位。
　　首先将<em>a</em>设为1，然后乘2，乘3，当乘到<em>n</em>时，即得到了<em>n</em>!的值。</p></blockquote>

<!-- more -->


<blockquote><p>输入格式</p>

<p>　　输入包含一个正整数<em>n</em>，<em>n</em>&lt;=1000。</p>

<p>输出格式</p>

<p>　　输出<em>n</em>!的准确值。</p>

<p>样例输入</p>

<p>10</p>

<p>样例输出</p>

<p>3628800</p></blockquote>

<p>这道题是典型的大数问题，难度不高，这篇的重点也不在于题目本身，只是记录和反思一下在编程中遇到的一个小问题。</p>

<h3>本人源码：</h3>

<pre><code class="c">#include &lt;iostream&gt;
using namespace std;
int main(){
    int n;
    cin&gt;&gt;n;
    int *p=new int [1000000];   //6
    p[0]=1;
    int q=0;;       //q 为最大的那一位
    int add=0;
    for (int i=1;i&lt;=n;i++){
        for (int j=0;j&lt;=q;j++){
            p[j]*=i;
            p[j]+=add;
            if (p[j]&gt;=10){
                add=p[j]/10;
                p[j]%=10;
            }
            else
                add=0;
            if (j==q&amp;&amp;add&gt;0){
                while (add){
                    p[++q]=add%10;
                    add/=10;
                }
                break;
            }

        }
        add=0;
    }


    for (;q&gt;=0;q--)
        cout&lt;&lt;p[q];
    cout&lt;&lt;endl;

    return 0;
}
</code></pre>

<p>程序流程很清晰，就是按题目的思路对每一位乘以n，然后对进位进行处理，由于我用了一个变量q来标注数的最高位，所以本程序中需要注意的一点就在于对最高位的进位的处理上，需要小小折腾一下。</p>

<p>网上其他博主的源码更为简洁，如下：</p>

<pre><code class="c">#include &lt;stdio.h&gt;  
#include &lt;stdlib.h&gt;  
#include &lt;string.h&gt;  
#define MAX 4000  
int main() {  
    int n,a[MAX],i,j,s,r=0;  
    scanf("%d",&amp;n);  
    memset(a,0,sizeof(a));  
    a[0]=1;  
    for(i=2;i&lt;=n;i++){  
        for(j=0;j&lt;MAX;j++){  
            s=a[j]*i+r;  
            r=s/10;//进位  
            a[j]=s%10;  
        }  
    }  
    for(i=MAX-1;i&gt;=0;i--){  
        if(a[i])  
            break;  
    }  
    for(j=i;j&gt;=0;j--)//倒序输出  
        printf("%d",a[j]);  
    return 0;  
}  
</code></pre>

<p>这是c语言版的，思路也非常清晰简洁，甚至简单粗暴而有效，可多加参考。</p>

<p>在本次解题过程中产生了一个小问题：</p>

<p>源码<code>int *p=new int [1000000];</code> 即初始数组的声明的时候，一开始写成了这样<code>int *p=new int (1000000);</code>。犯下了大错，  new int (1000000) 根本不是对数组的声明，而是对int 变量的声明，括号中的内容是int变量的初始值。所以在程序运行过程中产生了运行错误。<strong>以后需要多加注意</strong></p>

<p>在debug的过程中，发现了一个需要关注的地方，即【】静态数组与new 的动态数组的区别，其实之前也有所了解，即静态数组是和普通变量一样存储在栈里的，而动态数组（通过new关键字申请的内存空间）都是在heap堆中。</p>

<p>这是一个比较深入的问题，虽然这题的bug不在此处，但是这也是一个需要关注的地方，待后期深入学习后再撰写一篇博文详细记录。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[十六进制转八进制之大数问题的处理]]></title>
    <link href="https://mrfant.github.io/blog/2017/11/24/Hex2Oct/"/>
    <updated>2017-11-24T17:19:52+08:00</updated>
    <id>https://mrfant.github.io/blog/2017/11/24/Hex2Oct</id>
    <content type="html"><![CDATA[<blockquote><p><strong>问题描述</strong>
　　给定n个十六进制正整数，输出它们对应的八进制数。</p>

<p><strong>输入格式</strong>
　　输入的第一行为一个正整数n （1&lt;=n&lt;=10）。
　　接下来n行，每行一个由0~9、大写字母A~F组成的字符串，表示要转换的十六进制正整数，每个十六进制数长度不超过100000。</p>

<p><strong>输出格式</strong>
　　输出n行，每行为输入对应的八进制正整数。</p></blockquote>

<!-- more -->


<blockquote><p>　　<strong>【注意</strong>】
　　输入的十六进制数不会有前导0，比如012A。
　　输出的八进制数也不能有前导0。</p>

<p><strong>样例输入</strong>
　　2
　　39
　　123ABC</p>

<p><strong>样例输出</strong>
　　71
　　4435274</p>

<p>　　<strong>【提示</strong>】
　　先将十六进制数转换成某进制数，再由某进制数转换成八进制。</p></blockquote>

<p>这道题目的关键在于处理的数据位数，题目要求转换的十六进制数长度不超过十万位，十万位的十六进制数据是不可能由常规的数据类型来存储，只能存储为字符串类型。把输入存储为字符串类型后，就可以对其进行转换了，转换有以下几种思路：</p>

<p>一、将字符串中的每一位十六进制数转换为二进制数，再3位3位得将二进制数组合成八进制输出，这也是题目hints的思路，网上大多数都是以此为解。</p>

<p>二、第二个思路就是我自己所想的了，剑走偏锋，另辟蹊径。主要是：由于一位十六进制数的二进制为是四位，而八进制的二进制为是三位，所以3个十六进制数可以转换为4个八进制数。所以可以对输入的十六进制字符串3位3位得进行处理，转换为int类型的值，再在输出时利用规则漏洞<code>cout&lt;&lt;oct&lt;&lt;value;</code> 直接输出这3位十六进制数的八进制表示。这种方法算是一种“凑输出”的“作弊”手段吧，但是这毕竟是做题，而且算法结果确实能够提高效率，故不失为一种好方法。</p>

<h3>算法细节：</h3>

<p>前缀数的处理：</p>

<p>由于是对输入字符串的每3位进行处理，但是字符串位数不一定是3的倍数，所以需对字符串前面几个前缀数字进行特殊处理。</p>

<p>容易忽略的一点就是，在第二种方法中对字符串后面的每次三位处理的过程中由于是直接输出3位十六进制数的（先转换成int型）八进制表示，所以有可能出现漏输出0位的情况（如把0x1FF输出成“777”，正确的应该输出“0777”）为避免漏位，故需对该值进行分情况处理。我就是在这里摔了跤，忽略了细节。。。。</p>

<p>思路一源码：</p>

<pre><code class="c++">#include &lt;iostream&gt;
#include &lt;string&gt;
using namespace std;
int main()
{
    int n;
    cin&gt;&gt;n;
    for(int k=1;k&lt;=n;k++)
    {
        string s1,s2;//s1为输入的原始的十六进制串，s2为转化成的二进制串
        cin&gt;&gt;s1;
        s2="";//初始化
        for(int i=0;i&lt;s1.length();i++)//遍历，字符串上加上每一位
        {
            switch(s1[i])
            {
                case '0':s2+="0000";break;
                case '1':s2+="0001";break;
                case '2':s2+="0010";break;
                case '3':s2+="0011";break;
                case '4':s2+="0100";break;
                case '5':s2+="0101";break;
                case '6':s2+="0110";break;
                case '7':s2+="0111";break;
                case '8':s2+="1000";break;
                case '9':s2+="1001";break;
                case 'A':s2+="1010";break;
                case 'B':s2+="1011";break;
                case 'C':s2+="1100";break;
                case 'D':s2+="1101";break;
                case 'E':s2+="1110";break;
                case 'F':s2+="1111";break;
                default:break;
            }
        }
        int len=s2.length();

        if(len%3==1)//三个二进制为一位八进制，二进制串前面补0，确保二进制串的长度为3的倍数
            s2="00"+s2;
        else if(len%3==2)
            s2="0"+s2;
        int flag=0;//对前缀数字的处理
        for(int i=0;i&lt;s2.length();i+=3)
        {
            int num=4*(s2[i]-'0')+2*(s2[i+1]-'0')+(s2[i+2]-'0');
            if(num)
                flag=1;//忽略前导0
            if(flag)
                cout&lt;&lt;num;
        }
        cout&lt;&lt;endl;
    }
    return 0;
}
</code></pre>

<p>这是网上一博主的源码，比较具有代表性。</p>

<p>思路二源码：</p>

<pre><code class="c">#include &lt;iostream&gt;
//#include &lt;string&gt;
using namespace std;
int str2int(char c){
    if ((int)c&gt;='A')
        return (int)c-'A'+10;   //for A==10
    else return (int)c-'0';
}

int main(){
    //int str2int(char c);
    int n;
    cin&gt;&gt;n;
    string str;
    unsigned long strLength;
    int value;
    unsigned long prex;
    for (int i=0;i&lt;n;i++){
        cin&gt;&gt;str;
        strLength=str.length();
        //value=0;
        prex=strLength%3;
        //以下是对前面几个数字对特殊处理
        if (prex==2){
            value=str2int(str[0])*16+str2int(str[1]);
            cout&lt;&lt;oct&lt;&lt;value;
        }
        else if(prex==1){
            value=str2int(str[0]);
            cout&lt;&lt;oct&lt;&lt;value;
        }

        for (;prex&lt;strLength;prex+=3){
            //每3位处理
            value=str2int(str[prex])*16*16+str2int(str[prex+1])*16+str2int(str[prex+2]);
            if (prex != 0) {
                //补0以防止漏位
                // 0777的十进制表示为512，0077为 63 ，0007为 7
                // 故分别补0
                if (value&lt;512&amp;&amp;value&gt;63)
                    cout&lt;&lt;0;
                if (value &lt;=63&amp;&amp;value&gt;7)
                    cout&lt;&lt;"00";
                if (value&lt;=7)
                    cout &lt;&lt; "000";

            }
            cout&lt;&lt;oct&lt;&lt;value;
        }
        cout&lt;&lt;endl;

    }
    return 0;
}
</code></pre>

<p>两种方法输入系统的结果如下，方法二为第一条。</p>

<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1flt25pxwe7j30vc0680to.jpg" alt="对比" /></p>

<p>可见方法二在代码长度，cpu使用时间和内存使用上都比方法一优越。</p>

<p>但是这只是一个浅显的对比，两种算法都没有优化，在此仅提供一种思路，没有优劣之分。。。。。。。</p>

<h2>反思</h2>

<p>1、细节之处一定要多注意。</p>

<p>2、对蓝桥杯的输入输出的判罚机制有了更多了解，原来可以不用把输入全部输进去再输出结果，如本方法中的源码一般输入一条，输出一条也是可以的。</p>

<p>3、对本题方法一中的str.Length() 的 i为int型有点疑惑，因为之前总结出的蓝桥杯的编译系统为8位或16位，int类型应该存储不下十万这么大的数才对，但此处证明可以，有些疑惑，先挖个坑，到时候填。。。。。。。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[十六进制转十进制]]></title>
    <link href="https://mrfant.github.io/blog/2017/11/24/Hex2Dec/"/>
    <updated>2017-11-24T17:19:52+08:00</updated>
    <id>https://mrfant.github.io/blog/2017/11/24/Hex2Dec</id>
    <content type="html"><![CDATA[<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1flt24b3b6vj30zu0d2dhu.jpg" alt="问题描述" /></p>

<blockquote><p>问题描述</p>

<p>从键盘输入一个不超过8位的正的十六进制数字符串，将它转换为正的十进制数后输出。
　　注：十六进制数中的10~15分别用大写的英文字母A、B、C、D、E、F表示。</p></blockquote>

<!-- more -->


<p><strong>code</strong>：</p>

<pre><code class="c">#include &lt;iostream&gt;
using namespace std;
int 
main(void){
    unsigned long value;
    cin&gt;&gt;hex&gt;&gt;value;
    cout&lt;&lt;dec&lt;&lt;value;
    return 0;

}
</code></pre>

<p>题目本身不提，只是发现一个奇怪的情况：</p>

<p>测评程序测试一个四十亿的值时，显示出错。自然，在这种大数问题中我肯定考虑到了类型的范围的问题，所以，开始我使用的<code>value</code>变量是<code>Long</code>类型，然后惊奇地发现<code>Long</code>类型居然也无法容下这个四十亿的数，只有换成<code>unsigned Long</code>类型才能容纳。</p>

<p>一阵迷糊后才明白原来是因为蓝桥的测评环境与我本地的环境不同导致的。在不同位数的编译器环境中，int，long等类型占用的空间并不一样，c语言标准并没有严格规定，所以这可能会出现某些计算机中Long类型和其他计算机的int类型同样范围的情况。</p>

<p>这里先挖个坑，后面学习研究到的时候再来深入探讨。。。。。</p>
]]></content>
  </entry>
  
</feed>
